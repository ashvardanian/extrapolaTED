{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Prepare Wiki Texts\n",
            "\n",
            "Takes abstracts of English Wikipedia articles from the HuggingFace portal, repackaging them into Parquet files, and vectorizing all the abstract with the E5 model, resulting in 768-dimensional embeddings, and a USearch index on top of them. The outputs are:\n",
            "\n",
            "- `./data/ann-wiki-6m/title_abstract.parquet`\n",
            "- `./data/ann-wiki-6m/abstract.e5-base-v2.fbin`\n",
            "- `./data/ann-wiki-6m/abstract.e5-base-v2.usearch`"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "from datasets import load_dataset\n",
            "from prepare_embeddings import vectorize_e5, vectorize_uform"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data = load_dataset(\"wikipedia\", \"20220301.en\", cache_dir=\"./data/ann-wiki-6m/\")\n",
            "data, data[\"train\"][0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = data[\"train\"].to_pandas()\n",
            "df['abstract'] = df['text']\n",
            "df.pop('text')\n",
            "df.to_parquet('./data/ann-wiki-6m/title_abstract.parquet')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_batch = [data[\"train\"][i][\"text\"] for i in range(10)]\n",
            "test_batch, vectorize_e5(test_batch)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from usearch.io import load_matrix, save_matrix\n",
            "from tqdm import tqdm\n",
            "import os"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "vectors_path = \"./data/ann-wiki-6m/abstract.e5-base-v2.fbin\"\n",
            "if not os.path.exists(vectors_path):\n",
            "    matrix = np.zeros((6458670, 768), dtype=np.float32)\n",
            "    save_matrix(matrix, vectors_path)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Find all non-zero rows in the NumPy `matrix`, those rows we will need to vectorize.\n",
            "Iterate through the `data[\"train\"][i][\"text\"]` in batches, printing progress with `tqdm`.\n",
            "Vectorize using `vectorize_e5` function, that can take up to 1000 strings-list in a single batch.\n",
            "Once completed, or if terminated, call `save_matrix(matrix, vectors_path)`, to preserve progress."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "matrix = load_matrix(vectors_path)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "zero_rows = np.any(matrix == 0, axis=1)\n",
            "zero_indices = np.where(zero_rows)[0]\n",
            "len(zero_indices) / matrix.shape[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import time\n",
            "\n",
            "num_entries = len(data[\"train\"])\n",
            "start_idx = 0\n",
            "batch_size = 128\n",
            "last_save_time = time.time()\n",
            "save_interval = 600  # 10 minutes in seconds\n",
            "\n",
            "# Using tqdm for progress bar. The \"unit_scale\" and \"unit\" arguments allow us to track samples/second.\n",
            "with tqdm(total=len(zero_indices), unit=\"samples\") as pbar:\n",
            "    for i in range(0, len(zero_indices), batch_size):\n",
            "        batch_indices = zero_indices[i:i+batch_size]\n",
            "        batch_texts = [data[\"train\"][int(idx)][\"text\"] for idx in batch_indices]\n",
            "\n",
            "        # Vectorize using vectorize_e5 function\n",
            "        batch_vectors = vectorize_e5(batch_texts)\n",
            "\n",
            "        # Update the matrix\n",
            "        matrix[batch_indices] = batch_vectors\n",
            "\n",
            "        # Update the progress bar\n",
            "        pbar.update(len(batch_indices))\n",
            "\n",
            "        # Check if it's time to save the progress\n",
            "        current_time = time.time()\n",
            "        if current_time - last_save_time > save_interval:\n",
            "            save_matrix(matrix, vectors_path)\n",
            "            last_save_time = current_time\n",
            "\n",
            "# Save the matrix after the entire loop is finished\n",
            "save_matrix(matrix, vectors_path)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "vectors = matrix.astype(np.float16)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from usearch.index import Index\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "index_path = \"./data/ann-wiki-6m/abstract.e5-base-v2.usearch\"\n",
            "index = Index(dtype=\"f16\", metric=\"cos\", ndim=768)\n",
            "\n",
            "if os.path.exists(index_path):\n",
            "    index.load(index_path)\n",
            "else:\n",
            "    batch_size = 1000  # Adjust this based on your preference\n",
            "    total_batches = int(np.ceil(vectors.shape[0] / batch_size))\n",
            "\n",
            "    # Using tqdm for progress bar\n",
            "    for i in tqdm(range(total_batches), desc=\"Indexing batches\"):\n",
            "        start_idx = i * batch_size\n",
            "        end_idx = min((i + 1) * batch_size, vectors.shape[0])\n",
            "\n",
            "        batch_keys = np.arange(start_idx, end_idx)\n",
            "        batch_vectors = vectors[start_idx:end_idx]\n",
            "\n",
            "        index.add(batch_keys, batch_vectors)\n",
            "\n",
            "    index.save(index_path)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "py310",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.13"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
